# Science Trends
The code for this project is broken up into two repos:
* [arXiv-metadata](https://github.com/rocksonchang/arXiv-metadata) contains the code to pull data from the arXiv, and perform some pre-processing.
* [Science-Trends](https://github.com/rocksonchang/Science-Trends) contains the code to build bag-of-word and matrix factorization models.
---

As a researcher (both in physics and data science), a portion of my job description is to stay abreast of current trends in the field. With the volumes of papers published each year, it becomes nearly a full-time job to just keep up with my own domain, let alone beyond. Indeed, the [rate of article submission to the arXiv](https://arxiv.org/help/stats/2015_by_area/index) has continually grown since its launch in 1996. In the quantum physics sub-repository, over 5000 submissions were received in 2015 alone. 

I've long dreamt of a tool that would automatically parse the vast libraries of scientific literature to distill out trends such as the emergence of new and exciting topics. With the development of natural language processing techniques and the arXiv's participation in the [Open Access Initiative](https://arxiv.org/help/oa/index), such a tool may be realizable. 

This project is a first foray into the development of such a tool. It has a much more modest goal of tracking scientific trends as represented by the scientific literature. In particular, I focus on my own domain of quantum physics.


## Data source
The [arXiv](https://arxiv.org/) is an online repository of open-access academic publications, covering topics in Physics, Mathematics, Computer Science and much much more. The arXiv participates in the Open Access Initiative, allowing for bulk download of article metadata. The metadata for a submission consists of supporting information, such as:
* Article title
* Author list
* Submission date
* Subject keywords
* Final publication location
* Article abstract

This information is encoded in XML format. Using their API is simple:

```python
url = 'http://export.arxiv.org/oai2?verb=ListRecords&set=physics:quant-ph&metadataPrefix=oai_dc&{}'.format(queryDate)
rawData = urllib.urlopen(url).read()
```
The above call will return up to 1000 results at a time. A resumption token is provided at the end of the return if there are more than 1000 records available. This makes it possible to process large requests.
```python
url = 'http://export.arxiv.org/oai2?verb=ListRecords&resumptionToken={}'.format(token) 
```
The data I focus on here is the article title, abstract and the date of submission. The text data is cleaned up a bit by removing non alpha numeric characters (such as latex code) and common stopwords. These operations are performed using `obo`; many other packages do the same thing, for example NLTK.
```python
text_stripped = obo.stripNonAlphaNum(text)
text_cleaned = obo.removeStopwords(text_stripped,obo.stopwords)
```
Some effort was put towards lemmatisation, but in the end I found better results without it. 

The cleaned text is a document, and the complete collection of documents is referred to as a corpus.


## Approach 1: Word frequency analysis
A simple approach to extracting trends from the data is to simply look at the frequency of keywords as a function of time. To obtain time-dynamics, the data is grouped into quarterly blocks (3 months), and the most frequently occurring words in each quarter are identified. Here I choose to keep only the top 5000 words.
```python
doc_dictionary = obo.wordListToFreqDict(doc)
doc_sorteddict = obo.sortFreqDict(doc_dictionary)
topWords = doc_sorteddict[:5000])
```
Scores are generated by normalizing the word frequency by the number of submissions per quarter. The resulting score is thus a measure of the relative significance of a subject. 

In the app, you can browse a number of curated topics grouped by theme. One of my favorite themes is Breakthroughs which highlight some landmark publications that generated a significant and sustained level of interest in a new topic. You can also freely explore the data, entering your own topic key words. 

*Note that the app is currently hosted on Heroku. Please be patient as it takes a minute for the Heroku dynos to start up if they've been asleep. A refresh or two may be necessary. Migration to AWS coming ~~soon~~ someday!*

## Approach 2: Latent semantic indexing
A more sophisticated approach is to vectorize the corpus, providing a mathematical representation of the information. A number of different transformations can be applied to the corpus which highlight different aspects of the content/ These transformations and manipulations are performed with Gensim. The developers of Gensim have posted some great tutorials on how to use the package. 

There are three commonly used transformations that can be applied:
1. Documents can be converted into a bag-of-words representation, that simply counts the frequency of each word.
   ```python
   bow = dictionary.doc2bow(corpus)
   ```
2. A slightly more sophisticated representation is Tf-Idf, which assigns a weight to each word based on its relative importance.
   ```python
   tfidf = models.TfidfModel(corpus)
   corpus_tfidf = tfidf[corpus]
   ```
3. The Tf-Idf representation can be transformed into a new space of topic vectors using singular value decomposition, an approach referred to as Latent Semantic Indexing (LSI). These topic vectors consist of linear combinations of the original dictionary words.

   ```python
   lsi = models.LsiModel(corpus_tfidf,       
   id2word=dictionary, num_topics=Nfeat)
   corpus_lsi = lsi[corpus_tfidf]
   ```

   The singular values associated with these topics additionally represent the relative importance of a topic to the corpus. With this importance measure, dimensionality reduction can be performed, keeping only the most significant topics. The result is a light-weight representation of the full corpus.

The LSI approach is interesting since it lets the data (corpus) identify the important topics. Unfortunately, these topic vectors are notoriously difficult to interpret. Nonetheless, further work along this line seems like the way to have an unsupervised approach to identifying new and important trends. In addition to the promise of unsupervised learning, the vectorization of the corpus combined with the dimensionality reduction, allow for the efficient evaluation of document similarity for categorization. The most common similarity measure is a cosine similarity, which evaluates the inner product of two document vectors. 

The user interface for this app is still in development, but the source code for the implementation is available in the repo.

## Summary
Natural language processing (NLP) techniques allow for efficient analysis of large bodies of text. A variety of approaches are available, just a few of which have been touched on here (for example, I haven't even touched the subject of sentiment analysis). 

Considering the bulk of information stored as text, both in more traditional print media (digitisable with OCR techniques) and modern digital media, there are no lack of data sources and range of domains where such NLP approaches may have a significant impact. 

The above project represents a first foray into the exploration of the trends with. Evidently, with the richness of the dataset, a wide variety of features can be explored. One such goal is to try and detect the emergence of new subjects before they explode without having domain knowledge.